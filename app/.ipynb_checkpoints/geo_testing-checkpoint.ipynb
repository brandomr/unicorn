{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named elasticsearch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e46861ae3d60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0melasticsearch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mElasticsearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named elasticsearch"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def geo_endpoint():\n",
    "    query='China and United States'\n",
    "    url='http://localhost:9200/dossiers/_search'\n",
    "    q = {\n",
    "        \"fields\" : [\"file\"],\n",
    "        \"query\" : {\n",
    "            \"term\" : { \"file\" : query }\n",
    "            }\n",
    "        }\n",
    "    #r=requests.post(url,data=json.dumps(q))\n",
    "    r=es.search(body=q,index=DEFAULT_INDEX)\n",
    "    data=r\n",
    "    locations=[]\n",
    "    for hit in data['hits']['hits']:\n",
    "        for location in geodict_lib.find_locations_in_text(re.sub('\\s', ' ', str(hit['fields']['file']))):\n",
    "            for token in location['found_tokens']:\n",
    "                locations.append({'lat':token['lat'],'lon':token['lon'],'name':token['matched_string']})\n",
    "    \n",
    "    #geo=map(lambda x: x['found_tokens'])\n",
    "    return json.dumps(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'es' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f931808583ae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgeo_endpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-273813645883>\u001b[0m in \u001b[0;36mgeo_endpoint\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         }\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#r=requests.post(url,data=json.dumps(q))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDEFAULT_INDEX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mlocations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'es' is not defined"
     ]
    }
   ],
   "source": [
    "geo_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named elasticsearch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-dfb188690279>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mapp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflask_bcrypt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogin_manager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mapp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOrganization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjsonify\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrender_template\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murl_for\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/vagrant/unicorn/app/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflask\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFlask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBlueprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0melasticsearch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mElasticsearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mes_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mes_port\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madmin_username\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madmin_password\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_conn_str\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mflask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqlalchemy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSQLAlchemy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named elasticsearch"
     ]
    }
   ],
   "source": [
    "from app import app, es, db, flask_bcrypt, login_manager\n",
    "from app import User, Organization\n",
    "from flask import jsonify\n",
    "from flask import render_template\n",
    "from flask import url_for\n",
    "from flask import redirect\n",
    "from flask import Response\n",
    "from flask import request\n",
    "from flask import session\n",
    "from flask import abort\n",
    "from flask import flash\n",
    "from flask import send_file\n",
    "from flask.ext.login import (current_user, login_required, login_user,\n",
    "        logout_user, confirm_login, fresh_login_required)\n",
    "from flask import Blueprint\n",
    "\n",
    "from werkzeug import secure_filename\n",
    "import flask\n",
    "import tempfile\n",
    "import json\n",
    "from elasticsearch_dsl import Search, Q\n",
    "import io\n",
    "import re\n",
    "import magic\n",
    "import requests\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import os,sys\n",
    "import subprocess\n",
    "\n",
    "from bulk import bulk_download, bulk_search\n",
    "from config import tmp_dir\n",
    "from util.network import make_graph, document_graph\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import decomposition\n",
    "import corex as ce\n",
    "import numpy as np\n",
    "import phonenumbers\n",
    "from phonenumbers import geocoder\n",
    "\n",
    "parent = os.path.dirname(os.path.realpath(__file__))\n",
    "sys.path.append('/home/vagrant/geodict') #change path to MITIE top level\n",
    "\n",
    "import geodict_lib\n",
    "\n",
    "DEFAULT_INDEX = 'dossiers'\n",
    "uni = Blueprint('unicorn', __name__, url_prefix='/unicorn')\n",
    "\n",
    "\n",
    "@uni.route('/_bulk_search', methods=['POST'])\n",
    "@login_required\n",
    "def bulk_search_route():\n",
    "\n",
    "    search_results = request.form['searches']\n",
    "    searches = search_results.split('\\n')\n",
    "\n",
    "    data = bulk_search(searches)\n",
    "    return send_file(io.BytesIO(data.xls), as_attachment=True,\n",
    "            attachment_filename='bulk_{}.xls'.format(time.time()))\n",
    "\n",
    "    # Bulk search all of these queries\n",
    "    # Bundle results into excel\n",
    "\n",
    "\n",
    "@uni.route('/bulk_download')\n",
    "@login_required\n",
    "def bulk_download_route():\n",
    "    if 'last_query' not in session:\n",
    "        return abort(404)\n",
    "\n",
    "    last_query = session['last_query']\n",
    "\n",
    "    ids = last_query['ids']\n",
    "    query = last_query['query']\n",
    "\n",
    "    data = bulk_download(ids)\n",
    "\n",
    "    return send_file(io.BytesIO(data.xls), as_attachment=True,\n",
    "            attachment_filename=query + '.xls')\n",
    "\n",
    "@uni.route('/<doc_id>/debug')\n",
    "@login_required\n",
    "def request_doc(doc_id):\n",
    "    q = {\n",
    "            \"query\" : {\n",
    "                \"match\" : {\n",
    "                    \"_id\" : doc_id\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "    response = es.search(body=q, index=DEFAULT_INDEX)\n",
    "    return response\n",
    "\n",
    "def get_file(doc_id):\n",
    "    ''' Render base64 encoded contents of a given file by its doc_id '''\n",
    "    response = request_doc(doc_id)\n",
    "    try:\n",
    "        base64 = response['hits']['hits'][0]['_source']['file']\n",
    "        fn = response['hits']['hits'][0]['_source']['title']\n",
    "    except KeyError, IndexError:\n",
    "        abort(404)\n",
    "\n",
    "    return base64, fn\n",
    "\n",
    "@uni.route('/<doc_id>/entities')\n",
    "@login_required\n",
    "def get_entities(doc_id):\n",
    "    response = request_doc(doc_id)\n",
    "    try:\n",
    "        entities = response['hits']['hits'][0]['_source']['entities']\n",
    "    except KeyError, IndexError:\n",
    "        return jsonify([])\n",
    "\n",
    "    return jsonify({'entities': entities})\n",
    "\n",
    "@uni.route('/view/<doc_id>')\n",
    "@login_required\n",
    "def view_doc(doc_id):\n",
    "    ''' In-depth view of a particular document.\n",
    "    Displays pdf version of document, extracted entities,\n",
    "    as well as other analytics. '''\n",
    "\n",
    "    if is_owner_of_doc(doc_id):\n",
    "        return render_template('doc-view.html', doc_id=doc_id)\n",
    "    else:\n",
    "        return abort(403)\n",
    "\n",
    "@uni.route('/pdf/<doc_id>')\n",
    "@login_required\n",
    "def pdf_endpoint(doc_id):\n",
    "    base64, fn = get_file(doc_id)\n",
    "    b = base64.decode('base64')\n",
    "    mimetype = magic.from_buffer(b, mime=True)\n",
    "\n",
    "    if mimetype == 'application/pdf':\n",
    "        return send_file(io.BytesIO(b), as_attachment=True,\n",
    "                attachment_filename=fn)\n",
    "\n",
    "    else:\n",
    "        fd, fname = tempfile.mkstemp(prefix=tmp_dir)\n",
    "        stream = os.fdopen(fd, 'wb')\n",
    "        stream.write(b)\n",
    "        out_fname = fname + '.out'\n",
    "        stream.close()\n",
    "\n",
    "        os.chmod(fname, 0777)\n",
    "        try:\n",
    "            subprocess.check_output(['unoconv', '-o', out_fname, fname],\n",
    "                    stderr=subprocess.STDOUT)\n",
    "            with open(out_fname, 'rb') as converted_stream:\n",
    "                out = send_file(out_fname, as_attachment=True,\n",
    "                        attachment_filename=fn + '.pdf')\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print e.output\n",
    "            # Return error pdf\n",
    "            out = \"no pdf available\"\n",
    "\n",
    "\n",
    "        os.remove(fname)\n",
    "        os.remove(out_fname)\n",
    "\n",
    "        return out\n",
    "\n",
    "@uni.route('/topics/<doc_id>')\n",
    "def get_topics(doc_id):\n",
    "    topics=json.loads(open('/home/vagrant/unicorn/topics.json').read())\n",
    "    return json.dumps(topics['documents'][doc_id])\n",
    "\n",
    "@uni.route('/all_topics')\n",
    "def alltopics():\n",
    "    topics=json.loads(open('/home/vagrant/unicorn/topics.json').read())\n",
    "    docs=len(topics['documents'])\n",
    "    dist={}\n",
    "    for idx,x in enumerate(np.bincount([np.argmax(item[1]) for item in topics['documents'].items()])):\n",
    "        dist['topic'+str(idx+1)]=float(x)/docs\n",
    "    topics['dist']=dist\n",
    "    return json.dumps(topics)\n",
    "\n",
    "\n",
    "@uni.route('/download/<doc_id>')\n",
    "@login_required\n",
    "def download_endpoint(doc_id):\n",
    "\n",
    "    base64, fn = get_file(doc_id)\n",
    "    f = io.BytesIO(base64.decode('base64'))\n",
    "    return send_file(f,\n",
    "            as_attachment=True,\n",
    "            attachment_filename=fn)\n",
    "\n",
    "@uni.route('/search/<query>/<page>/preserve')\n",
    "@login_required\n",
    "def search_preserve(query, page):\n",
    "    return search_endpoint(query, page, box_only=True)\n",
    "\n",
    "@uni.route('/search')\n",
    "@uni.route('/search/<query>')\n",
    "@uni.route('/search/<query>/<page>')\n",
    "@login_required\n",
    "def search_endpoint(query=None, page=None, box_only=False):\n",
    "    if not query and not page:\n",
    "        last_query = session.get('last_query', None)\n",
    "        if last_query:\n",
    "            query, page = last_query['query'], last_query['page']\n",
    "        else:\n",
    "            # better error\n",
    "            return abort(404)\n",
    "\n",
    "    if not page:\n",
    "        page = 1\n",
    "\n",
    "    session['last_query'] = {'query': query, 'page': page, 'ids': []}\n",
    "    # convert pages to records for ES\n",
    "    start = int(page)\n",
    "    if start > 1:\n",
    "        start *= 10\n",
    "\n",
    "    q = {\n",
    "            \"fields\": [\"title\", \"highlight\", \"entities\", \"owner\"],\n",
    "            \"from\": start,\n",
    "            \"query\" : {\n",
    "                \"match\" : {\n",
    "                    \"file\" : query\n",
    "                    }\n",
    "                },\n",
    "\n",
    "            \"highlight\": { \"fields\": { \"file\": { } },\n",
    "                \"pre_tags\" : [\"<span class='highlight'>\"],\n",
    "                \"post_tags\" : [\"</span>\"]\n",
    "                }\n",
    "            }\n",
    "    raw_response = es.search(body=q, index=DEFAULT_INDEX,\n",
    "            df=\"file\",\n",
    "            size=10)\n",
    "\n",
    "    hits = []\n",
    "\n",
    "    for resp in raw_response['hits']['hits']:\n",
    "        # Store returned ids\n",
    "        session['last_query']['ids'].append(resp['_id'])\n",
    "\n",
    "        if is_owner(resp['fields']['owner'][0]):\n",
    "            # Flatten structure for individual hits\n",
    "            hits.append({'id': resp['_id'],\n",
    "                'title': resp['fields']['title'][0],\n",
    "                'highlight': resp['highlight']['file'][0],\n",
    "                'permissions': True\n",
    "                })\n",
    "        else:\n",
    "            hits.append({'id': resp['_id'],\n",
    "                'title': resp['fields']['title'][0],\n",
    "                'permissions': False\n",
    "                })\n",
    "\n",
    "\n",
    "    results = {\n",
    "            'hits': hits,\n",
    "            'took': float(raw_response['took'])/1000,\n",
    "            'total': \"{:,}\".format(raw_response['hits']['total']),\n",
    "            'total_int': int(raw_response['hits']['total']),\n",
    "            'query': query,\n",
    "            'from': int(page)\n",
    "            }\n",
    "\n",
    "    if box_only:\n",
    "        return render_template('search-results-box.html', results=results)\n",
    "\n",
    "    return render_template('search-template.html', results=results)\n",
    "\n",
    "@uni.route('/')\n",
    "@login_required\n",
    "def root():\n",
    "    user_struct = {\n",
    "            'notifs': 0\n",
    "            }\n",
    "    return render_template('index-dash.html', user=user_struct)\n",
    "\n",
    "@uni.route('/user')\n",
    "@login_required\n",
    "def user_page():\n",
    "    # If current user is admin\n",
    "\n",
    "    return render_template('user-invite.html')\n",
    "\n",
    "\n",
    "@uni.route('/upload/')\n",
    "@login_required\n",
    "def upload_form():\n",
    "    return render_template('upload.html')\n",
    "\n",
    "@uni.route('/_upload-documents', methods=['POST'])\n",
    "@login_required\n",
    "def upload_endpoint():\n",
    "    files = request.files.getlist('file[]')\n",
    "    d = {}\n",
    "    for f in files:\n",
    "        sf = secure_filename(f.filename)\n",
    "\n",
    "        es_dict = {\n",
    "                'file': f.read().encode('base64'),\n",
    "                'title': sf,\n",
    "                'owner': 'blank' #current_owner.organization.organization\n",
    "                }\n",
    "        es.index(index=DEFAULT_INDEX, doc_type='attachment', body=es_dict)\n",
    "        f.close()\n",
    "\n",
    "    return redirect(url_for('.root'))\n",
    "\n",
    "@uni.route('/viz')\n",
    "@login_required\n",
    "def viz_all():\n",
    "    q = {\n",
    "        \"fields\" : [\"entities\",\"title\"],\n",
    "        \"query\" : {\n",
    "            \"match_all\" : {}\n",
    "            },\n",
    "        \"size\": 100\n",
    "        }\n",
    "    r = es.search(body=q, index=DEFAULT_INDEX)\n",
    "    graph = document_graph(r['hits']['hits'])\n",
    "\n",
    "    return json.dumps(graph)\n",
    "\n",
    "\n",
    "@uni.route('/geo')\n",
    "@login_required\n",
    "def geo_endpoint():\n",
    "    query=session['last_query']['query']\n",
    "    url='http://localhost:9200/dossiers/_search'\n",
    "    q = {\n",
    "        \"fields\" : [\"file\"],\n",
    "        \"query\" : {\n",
    "            \"term\" : { \"file\" : query }\n",
    "            }\n",
    "        }\n",
    "    #r=requests.post(url,data=json.dumps(q))\n",
    "    r=es.search(body=q,index=DEFAULT_INDEX)\n",
    "    data=r\n",
    "    locations=[]\n",
    "    for hit in data['hits']['hits']:\n",
    "        for location in geodict_lib.find_locations_in_text(re.sub('\\s', ' ', str(hit['fields']['file']))):\n",
    "            for token in location['found_tokens']:\n",
    "                locations.append({'lat':token['lat'],'lon':token['lon'],'name':token['matched_string']})\n",
    "    \n",
    "    #geo=map(lambda x: x['found_tokens'])\n",
    "    return json.dumps(locations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@uni.route('/viz/<query>')\n",
    "@login_required\n",
    "def viz_endpoint(query):\n",
    "    url='http://localhost:9200/dossiers/_search'\n",
    "    q = {\n",
    "        \"_source\": [\"entity\"],\n",
    "        \"fields\" : [\"entities\",\"title\"],\n",
    "        \"query\" : {\n",
    "            \"term\" : { \"file\" : query }\n",
    "            },\n",
    "        \"size\": 100\n",
    "        }\n",
    "    #r=requests.post(url,data=json.dumps(q))\n",
    "    r=es.search(body=q,index=DEFAULT_INDEX)\n",
    "    data=r\n",
    "    #graph = make_graph(data)\n",
    "    graph = document_graph(data['hits']['hits'])\n",
    "    return json.dumps(graph)\n",
    "\n",
    "@uni.route('/viz_latest')\n",
    "@login_required\n",
    "def viz_latest():\n",
    "    return viz_endpoint(session['last_query']['query'])\n",
    "\n",
    "\n",
    "@uni.route('/wc_latest')\n",
    "@login_required\n",
    "def wc_latest():\n",
    "    return wc(session['last_query']['query'])\n",
    "\n",
    "@uni.route('/url_list')\n",
    "@uni.route('/url_list/<query>')\n",
    "@login_required\n",
    "def url_fetch(query=\"\"):\n",
    "    #query=\"http\"\n",
    "    if not query:\n",
    "        query=session['last_query']['query']\n",
    "    stopset=set(stopwords.words('english'))\n",
    "    url='http://localhost:9200/dossiers/_search'\n",
    "    q = {\n",
    "        \"fields\" : [\"file\"],\n",
    "        \"query\" : {\n",
    "            \"term\" : { \"file\" : query }\n",
    "            }\n",
    "        }\n",
    "    #r=requests.post(url,data=json.dumps(q))    \n",
    "    r=es.search(body=q,index=DEFAULT_INDEX)\n",
    "    data=r['hits']['hits']\n",
    "    urls=[]\n",
    "    pn=[]\n",
    "    for doc in data:\n",
    "        urls.append(re.findall(r'(https?://[^\\s]+)', doc['fields']['file'][0]))\n",
    "        try:\n",
    "            for match in phonenumbers.PhoneNumberMatcher(doc['fields']['file'][0], region=None):\n",
    "                    pn.append({'number':phonenumbers.format_number(match.number, phonenumbers.PhoneNumberFormat.E164),'location':geocoder.description_for_number(match.number,\"en\")})     \n",
    "        except KeyError:\n",
    "            pass\n",
    "    urls=filter(lambda x: x!=[],urls)\n",
    "    #urls_flat=reduce(lambda x,y: x.extend(y),urls)\n",
    "    urls_flat=[item for sublist in urls for item in sublist]\n",
    "    return json.dumps({'urls':dict(Counter(urls_flat)), 'pn':pn})\n",
    "\n",
    "@uni.route('/wordcloud/<query>')\n",
    "@login_required\n",
    "def wc(query):\n",
    "    stopset=set(stopwords.words('english'))\n",
    "    url='http://localhost:9200/dossiers/_search'\n",
    "    q = {\n",
    "        \"fields\" : [\"file\"],\n",
    "        \"query\" : {\n",
    "            \"term\" : { \"file\" : query }\n",
    "            }\n",
    "        }\n",
    "    #r=requests.post(url,data=json.dumps(q))\n",
    "    r=es.search(body=q,index=DEFAULT_INDEX)\n",
    "    data=r['hits']['hits'][0]['fields']['file'][0]\n",
    "\n",
    "    nowhite=re.sub('\\s', ' ', data)\n",
    "    nowhite=re.sub(r'[^\\w\\s]', '', data)\n",
    "    wt=word_tokenize(nowhite)\n",
    "    wc=dict(Counter(wt))\n",
    "    frequency=[]\n",
    "    for k,v in wc.iteritems():\n",
    "        frequency.append(dict({\"text\":k,\"size\":v*3}))\n",
    "    frequency=filter(lambda x:x['size']>3 and x['text'].lower() not in stopset,frequency)\n",
    "    return json.dumps(frequency)\n",
    "\n",
    "@uni.route('/topicmodel')\n",
    "@uni.route('/topicmodel/<query>')\n",
    "@login_required\n",
    "def tm(query):\n",
    "    #count_vectorizer.fit_transform(train_set)\n",
    "    #print \"Vocabulary:\", count_vectorizer.vocabulary\n",
    "    # Vocabulary: {'blue': 0, 'sun': 1, 'bright': 2, 'sky': 3}\n",
    "    #freq_term_matrix = count_vectorizer.transform(test_set)\n",
    "    #print freq_term_matrix.todense()\n",
    "    stopset=set(stopwords.words('english'))\n",
    "    url='http://localhost:9200/dossiers/_search'\n",
    "    q = {\n",
    "        \"fields\" : [\"file\"],\n",
    "        \"query\" : {\n",
    "            \"term\" : { \"file\" : query }\n",
    "            }\n",
    "        }\n",
    "    \n",
    "  \n",
    "    return json.dumps(topic_words[0])\n",
    "\n",
    "\n",
    "@uni.route('/<doc_id>/related')\n",
    "@login_required\n",
    "def more_like_this(doc_id):\n",
    "    ''' Returns similar documents '''\n",
    "    q = {\n",
    "      \"fields\": [\"title\"],\n",
    "        \"query\": {\n",
    "            \"more_like_this\" : {\n",
    "            \"docs\" : [\n",
    "            {\n",
    "                \"_index\" : \"dossiers\",\n",
    "                \"_type\" : \"attachment\",\n",
    "                \"_id\" : doc_id\n",
    "            }]\n",
    "          }\n",
    "        },\n",
    "        \"size\": 10\n",
    "    }\n",
    "\n",
    "    response = es.search(body=q, index=DEFAULT_INDEX)\n",
    "    results = {'results': []}\n",
    "    try:\n",
    "        for r in response['hits']['hits']:\n",
    "            results['results'].append({\n",
    "                'id': r['_id'],\n",
    "                'name': r['fields']['title'][0]\n",
    "                })\n",
    "    except KeyError, IndexError:\n",
    "        pass\n",
    "\n",
    "    return jsonify(results)\n",
    "\n",
    "@uni.route('/index.html')\n",
    "@login_required\n",
    "def reroute_index():\n",
    "    return redirect(url_for('.root'))\n",
    "\n",
    "@uni.route('/register', methods=['POST'])\n",
    "@login_required\n",
    "def handle_registration():\n",
    "    if not current_user.moderator:\n",
    "        return redirect(url_for('.root'))\n",
    "\n",
    "    email = request.form['email']\n",
    "    password = request.form['password']\n",
    "\n",
    "    # Change this to: current user's group\n",
    "    organization = current_user.organization\n",
    "\n",
    "    pw_hashed = flask_bcrypt.generate_password_hash(password)\n",
    "    new_user = User(email=email,\n",
    "                    password=pw_hashed,\n",
    "                    organization=organization)\n",
    "\n",
    "    db.session.add(new_user)\n",
    "    db.session.commit()\n",
    "\n",
    "    return redirect(url_for('.root'))\n",
    "\n",
    "######################################\n",
    "# Registration blueprint:\n",
    "######################################\n",
    "\n",
    "@uni.route(\"/login\", methods=[\"GET\", \"POST\"])\n",
    "def login():\n",
    "    if request.method == 'GET':\n",
    "        return render_template(\"/user-login.html\")\n",
    "\n",
    "    email = request.form[\"email\"]\n",
    "    password = request.form[\"password\"]\n",
    "    user = User.query.filter_by(email=email).first()\n",
    "    if user and flask_bcrypt.check_password_hash(user.password,\n",
    "            password):\n",
    "            if login_user(user):\n",
    "                return redirect(url_for('.root'))\n",
    "            else:\n",
    "                flash(\"Invalid email or password\")\n",
    "\n",
    "    return render_template(\"/user-login.html\")\n",
    "\n",
    "@uni.route('/logout')\n",
    "@login_required\n",
    "def logout():\n",
    "    logout_user()\n",
    "    return redirect(url_for('.login'))\n",
    "\n",
    "\n",
    "@login_manager.unauthorized_handler\n",
    "def login_redirect():\n",
    "    return redirect(url_for('.login'))\n",
    "\n",
    "@login_manager.user_loader\n",
    "def load_user(userid):\n",
    "    return User.query.get(userid)\n",
    "\n",
    "def is_owner_of_doc(doc):\n",
    "    owner = es.get(index=DEFAULT_INDEX, doc_type='attachment', id=doc,\n",
    "            fields='owner')['fields']['owner'][0]\n",
    "    return is_owner(owner)\n",
    "\n",
    "def is_owner(org):\n",
    "    return current_user.organization.organization == 'admins' or \\\n",
    "            current_user.organization.organization == org\n",
    "\n",
    "@app.errorhandler(403)\n",
    "def permission_denied(e):\n",
    "    return render_template('permission-denied.html'), 403\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/home/vagrant/anaconda/lib/python27.zip',\n",
       " '/home/vagrant/anaconda/lib/python2.7',\n",
       " '/home/vagrant/anaconda/lib/python2.7/plat-linux2',\n",
       " '/home/vagrant/anaconda/lib/python2.7/lib-tk',\n",
       " '/home/vagrant/anaconda/lib/python2.7/lib-old',\n",
       " '/home/vagrant/anaconda/lib/python2.7/lib-dynload',\n",
       " '/home/vagrant/.local/lib/python2.7/site-packages',\n",
       " '/home/vagrant/anaconda/lib/python2.7/site-packages',\n",
       " '/home/vagrant/anaconda/lib/python2.7/site-packages/Sphinx-1.2.3-py2.7.egg',\n",
       " '/home/vagrant/anaconda/lib/python2.7/site-packages/cryptography-0.8-py2.7-linux-x86_64.egg',\n",
       " '/home/vagrant/anaconda/lib/python2.7/site-packages/setuptools-14.3-py2.7.egg',\n",
       " '/home/vagrant/anaconda/lib/python2.7/site-packages/IPython/extensions']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
